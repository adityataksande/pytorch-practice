{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim = 1)\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.rand((28, 28))\n",
    "X = X.view(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9000, 0.1665, 0.4150, 0.3786, 0.7605, 0.9374, 0.5741, 0.9877, 0.6118,\n",
       "        0.7336, 0.1307, 0.2580, 0.7355, 0.0367, 0.7604, 0.9343, 0.0599, 0.3981,\n",
       "        0.2454, 0.5602, 0.8799, 0.2017, 0.1747, 0.3045, 0.1147, 0.7156, 0.2317,\n",
       "        0.7840, 0.1358, 0.2912, 0.2422, 0.4500, 0.2963, 0.8793, 0.0322, 0.2072,\n",
       "        0.7095, 0.6580, 0.6795, 0.5497, 0.9890, 0.9816, 0.7644, 0.9666, 0.3624,\n",
       "        0.7134, 0.3017, 0.7180, 0.1836, 0.7728, 0.1415, 0.8671, 0.4501, 0.7466,\n",
       "        0.5559, 0.6874, 0.2497, 0.8179, 0.0483, 0.6816, 0.9270, 0.0470, 0.3863,\n",
       "        0.3850, 0.2854, 0.6595, 0.5373, 0.1710, 0.7079, 0.1537, 0.9319, 0.8496,\n",
       "        0.2876, 0.5940, 0.4844, 0.4150, 0.6669, 0.8707, 0.8326, 0.9538, 0.1105,\n",
       "        0.4483, 0.3316, 0.1372, 0.6255, 0.7105, 0.1894, 0.0965, 0.3066, 0.6774,\n",
       "        0.5814, 0.2908, 0.8380, 0.6884, 0.5044, 0.6961, 0.4162, 0.8658, 0.7357,\n",
       "        0.4971, 0.6189, 0.5398, 0.7492, 0.1915, 0.7434, 0.7372, 0.2103, 0.0427,\n",
       "        0.9652, 0.7923, 0.7319, 0.5259, 0.2424, 0.9423, 0.0411, 0.0262, 0.8182,\n",
       "        0.7061, 0.3236, 0.0838, 0.0778, 0.3351, 0.4470, 0.2967, 0.3747, 0.8209,\n",
       "        0.7068, 0.4087, 0.3527, 0.8957, 0.1676, 0.5879, 0.2793, 0.2765, 0.6646,\n",
       "        0.3252, 0.7520, 0.8136, 0.9205, 0.0721, 0.9542, 0.3652, 0.7524, 0.0846,\n",
       "        0.9994, 0.7366, 0.5920, 0.0332, 0.8049, 0.0545, 0.3719, 0.6555, 0.4698,\n",
       "        0.5304, 0.8388, 0.9980, 0.0540, 0.4216, 0.0759, 0.2597, 0.9217, 0.2324,\n",
       "        0.9920, 0.3269, 0.0667, 0.0026, 0.9550, 0.0745, 0.4496, 0.7754, 0.6330,\n",
       "        0.7314, 0.9894, 0.1810, 0.7006, 0.5164, 0.5666, 0.7052, 0.2034, 0.1054,\n",
       "        0.1639, 0.0409, 0.0699, 0.9467, 0.1359, 0.0869, 0.3615, 0.8450, 0.6251,\n",
       "        0.7661, 0.2607, 0.0178, 0.4061, 0.5531, 0.3008, 0.7290, 0.2662, 0.7665,\n",
       "        0.8038, 0.2739, 0.2749, 0.1878, 0.6075, 0.6865, 0.0080, 0.5048, 0.2589,\n",
       "        0.3274, 0.9737, 0.1581, 0.2053, 0.5633, 0.2915, 0.8176, 0.3671, 0.3774,\n",
       "        0.3099, 0.8582, 0.0299, 0.4767, 0.7516, 0.6458, 0.1731, 0.3527, 0.7921,\n",
       "        0.7602, 0.0164, 0.4102, 0.3598, 0.0106, 0.1893, 0.8840, 0.7778, 0.9716,\n",
       "        0.4675, 0.4662, 0.5961, 0.1323, 0.8088, 0.8828, 0.3176, 0.7230, 0.9469,\n",
       "        0.3937, 0.1107, 0.3271, 0.8134, 0.3070, 0.8035, 0.0102, 0.6302, 0.7824,\n",
       "        0.7111, 0.4380, 0.6319, 0.4468, 0.2316, 0.4074, 0.2480, 0.3568, 0.3335,\n",
       "        0.6135, 0.8054, 0.4795, 0.7500, 0.0482, 0.9708, 0.2009, 0.8461, 0.8358,\n",
       "        0.5554, 0.0297, 0.5302, 0.9674, 0.5305, 0.7167, 0.6209, 0.5879, 0.6087,\n",
       "        0.4892, 0.1301, 0.6230, 0.7981, 0.6991, 0.0661, 0.1382, 0.3300, 0.1330,\n",
       "        0.1509, 0.2944, 0.4276, 0.7955, 0.5386, 0.2425, 0.3685, 0.2005, 0.0386,\n",
       "        0.3012, 0.4948, 0.3756, 0.5956, 0.2931, 0.7599, 0.6925, 0.4579, 0.3733,\n",
       "        0.2885, 0.5658, 0.5222, 0.4277, 0.0089, 0.4541, 0.2184, 0.9448, 0.9328,\n",
       "        0.9558, 0.9772, 0.7908, 0.1938, 0.9168, 0.5088, 0.8743, 0.3808, 0.6083,\n",
       "        0.3722, 0.9728, 0.3414, 0.9363, 0.8471, 0.0625, 0.9228, 0.4919, 0.5474,\n",
       "        0.5719, 0.3135, 0.1184, 0.1249, 0.0142, 0.6038, 0.3236, 0.4691, 0.5830,\n",
       "        0.1944, 0.2720, 0.7897, 0.4418, 0.5969, 0.9207, 0.0374, 0.7905, 0.7251,\n",
       "        0.0528, 0.0930, 0.1918, 0.9156, 0.7246, 0.8029, 0.0398, 0.7403, 0.4885,\n",
       "        0.6677, 0.3320, 0.6826, 0.7312, 0.4975, 0.7057, 0.0154, 0.9730, 0.8486,\n",
       "        0.6991, 0.7103, 0.5371, 0.3464, 0.9329, 0.4523, 0.1014, 0.3519, 0.5309,\n",
       "        0.9938, 0.1702, 0.0467, 0.9378, 0.4407, 0.9127, 0.0356, 0.9872, 0.8309,\n",
       "        0.3234, 0.6301, 0.5335, 0.9784, 0.4444, 0.5041, 0.2431, 0.9036, 0.4334,\n",
       "        0.6725, 0.2954, 0.0766, 0.3068, 0.5765, 0.1995, 0.8977, 0.0882, 0.4114,\n",
       "        0.1652, 0.9101, 0.6774, 0.6161, 0.0304, 0.8485, 0.9112, 0.9082, 0.6391,\n",
       "        0.3263, 0.9431, 0.0186, 0.8784, 0.8202, 0.6147, 0.2138, 0.8734, 0.1478,\n",
       "        0.9597, 0.2499, 0.8539, 0.1990, 0.0101, 0.0316, 0.1269, 0.9625, 0.2695,\n",
       "        0.4912, 0.0109, 0.4904, 0.4323, 0.2506, 0.1880, 0.1120, 0.5508, 0.5396,\n",
       "        0.1264, 0.4781, 0.4296, 0.5116, 0.7978, 0.9703, 0.1494, 0.6980, 0.9667,\n",
       "        0.1343, 0.1991, 0.0850, 0.5954, 0.3042, 0.6598, 0.4289, 0.9507, 0.5425,\n",
       "        0.3235, 0.3964, 0.5579, 0.3703, 0.4599, 0.7022, 0.9583, 0.5178, 0.3758,\n",
       "        0.0896, 0.1131, 0.6629, 0.3274, 0.9947, 0.3670, 0.2556, 0.8393, 0.3822,\n",
       "        0.4107, 0.1313, 0.6997, 0.0903, 0.9708, 0.1474, 0.9408, 0.8323, 0.8071,\n",
       "        0.0031, 0.0023, 0.3817, 0.5952, 0.0115, 0.3670, 0.4487, 0.8381, 0.7848,\n",
       "        0.9428, 0.9046, 0.4870, 0.4495, 0.4598, 0.6419, 0.9783, 0.7717, 0.0547,\n",
       "        0.5448, 0.1430, 0.4976, 0.7230, 0.9666, 0.6176, 0.3547, 0.1751, 0.8651,\n",
       "        0.4723, 0.5431, 0.2134, 0.9812, 0.3350, 0.4230, 0.3864, 0.0623, 0.9175,\n",
       "        0.5696, 0.6400, 0.1408, 0.5155, 0.5712, 0.2599, 0.1136, 0.8063, 0.3364,\n",
       "        0.5719, 0.1396, 0.6211, 0.8142, 0.8781, 0.2137, 0.7888, 0.3151, 0.1704,\n",
       "        0.3552, 0.3892, 0.3000, 0.8763, 0.5039, 0.4121, 0.8828, 0.9347, 0.5291,\n",
       "        0.0407, 0.0500, 0.4767, 0.9384, 0.5377, 0.3208, 0.8120, 0.0249, 0.7115,\n",
       "        0.1408, 0.9271, 0.9783, 0.4554, 0.7480, 0.5639, 0.8760, 0.7396, 0.7372,\n",
       "        0.7314, 0.5687, 0.0514, 0.9845, 0.2067, 0.6410, 0.1772, 0.8637, 0.9866,\n",
       "        0.5463, 0.5589, 0.2156, 0.7499, 0.2073, 0.6850, 0.3333, 0.3511, 0.8931,\n",
       "        0.7779, 0.2385, 0.8775, 0.7961, 0.8110, 0.5368, 0.4297, 0.0438, 0.8651,\n",
       "        0.6712, 0.4572, 0.3587, 0.1872, 0.8777, 0.4109, 0.0902, 0.8234, 0.7824,\n",
       "        0.1574, 0.2938, 0.7709, 0.4985, 0.9418, 0.6390, 0.7127, 0.6504, 0.3407,\n",
       "        0.1028, 0.6064, 0.6270, 0.8877, 0.8810, 0.3482, 0.5214, 0.6029, 0.6641,\n",
       "        0.4483, 0.5517, 0.9327, 0.4515, 0.7905, 0.3223, 0.7713, 0.2596, 0.3026,\n",
       "        0.2897, 0.4892, 0.1021, 0.9616, 0.4423, 0.4530, 0.1170, 0.7864, 0.7648,\n",
       "        0.4307, 0.6090, 0.6771, 0.6058, 0.8899, 0.7592, 0.1460, 0.4377, 0.4162,\n",
       "        0.1709, 0.9715, 0.1095, 0.8328, 0.4234, 0.3444, 0.2591, 0.7843, 0.8125,\n",
       "        0.7035, 0.5294, 0.9355, 0.5271, 0.8956, 0.3833, 0.9880, 0.7753, 0.1823,\n",
       "        0.3920, 0.8628, 0.0715, 0.7748, 0.5982, 0.4729, 0.4321, 0.1654, 0.1696,\n",
       "        0.5659, 0.2839, 0.2047, 0.4843, 0.7099, 0.5269, 0.6631, 0.2817, 0.8683,\n",
       "        0.5950, 0.8699, 0.5022, 0.0514, 0.8377, 0.2920, 0.0078, 0.7557, 0.7568,\n",
       "        0.9820, 0.3436, 0.0442, 0.8254, 0.4311, 0.8269, 0.9178, 0.1751, 0.7845,\n",
       "        0.9368, 0.0231, 0.6285, 0.8403, 0.3510, 0.1925, 0.1555, 0.1309, 0.5842,\n",
       "        0.6949, 0.2804, 0.3786, 0.3295, 0.0796, 0.0714, 0.9308, 0.2498, 0.5399,\n",
       "        0.6700, 0.1455, 0.5456, 0.0415, 0.2828, 0.0327, 0.1294, 0.4688, 0.9969,\n",
       "        0.7625, 0.1260, 0.1633, 0.4532, 0.7130, 0.1937, 0.3669, 0.3989, 0.9393,\n",
       "        0.5175, 0.2830, 0.3231, 0.9244, 0.6166, 0.6508, 0.8620, 0.5227, 0.6493,\n",
       "        0.1120, 0.1051, 0.7449, 0.2430, 0.2315, 0.8610, 0.1772, 0.8073, 0.4543,\n",
       "        0.1116, 0.7619, 0.6668, 0.3486, 0.5518, 0.9402, 0.2310, 0.8649, 0.2620,\n",
       "        0.8054, 0.9262, 0.8755, 0.2552, 0.1339, 0.3725, 0.9351, 0.0949, 0.0013,\n",
       "        0.3449, 0.7306, 0.8218, 0.3495, 0.3954, 0.9904, 0.5076, 0.3021, 0.0410,\n",
       "        0.3272])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3258, -2.2932, -2.3684, -2.1787, -2.2029, -2.3297, -2.3537, -2.4729,\n",
       "         -2.2075, -2.3290]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "train = datasets.MNIST(\"\",train=True, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\",train=False, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([4, 0, 3, 1, 1, 8, 7, 8, 1, 2])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2793, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2587, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2405, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimzer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    # data is the batch of fetures sets anf=d Labels\n",
    "    X, y = data\n",
    "    net.zero_grad()\n",
    "    output = net(X.view(-1,28*28))\n",
    "    loss = F.nll_loss(output, y)\n",
    "    loss.backward()\n",
    "    optimzer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.098\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1 \n",
    "            total += 1 \n",
    "print(\"Accuracy : \", round(correct/total,3))            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOnUlEQVR4nO3de7CU9X3H8c8HCuggNuAtqEy8hNTYGzan0ESb2tKoMVa0qRmZjiGthtTRRBOn1epk9I+mY1svNfXSwcuEpEZrahJttYkMY2sypsSjBYUg4AUVQVAZoxbF4znf/nHW9ohnf3vYffYC3/dr5szuPt999vnOM3x4dvf3PPtzRAjA7m9ctxsA0BmEHUiCsANJEHYgCcIOJPELndzYRE+KPTS5k5sEUnlT/6O3YrtHq7UUdtsnSLpG0nhJN0XE5aXn76HJmuO5rWwSQMGyWFq31vTbeNvjJV0n6ZOSjpQ03/aRzb4egPZq5TP7bElPRMRTEfGWpNslzaumLQBVayXsB0l6bsTjDbVl72J7oe1+2/0D2t7C5gC0opWwj/YlwHvOvY2IRRHRFxF9EzSphc0BaEUrYd8gacaIxwdL2thaOwDapZWwPyRppu1DbU+UdLqku6tpC0DVmh56i4i3bZ8r6YcaHnq7JSJWVdYZgEq1NM4eEfdKureiXgC0EafLAkkQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kERLs7iiN3jCxLq1ob4PF9d94vQ9i/Wz5y4p1s+furZYf2nwjbq1eV/98+K6+3xnRbE+tG1bsY53aynsttdLek3SoKS3I6KviqYAVK+KI/vvRsRLFbwOgDbiMzuQRKthD0n32X7Y9sLRnmB7oe1+2/0D2t7i5gA0q9W38UdHxEbb+0taYvvxiHhg5BMiYpGkRZK0t6dFi9sD0KSWjuwRsbF2u0XS9yTNrqIpANVrOuy2J9ue8s59ScdJWllVYwCq5Yjm3lnbPkzDR3Np+OPAtyPia6V19va0mOO5TW0P9a29of4bqsdPvq6l1x7X4HgwpKGWXr/k4hfmFOsrP9K+be+qlsVSvRpbPVqt6c/sEfGUpF9vuisAHcXQG5AEYQeSIOxAEoQdSIKwA0lwiWsPKF2iKklrvj6rWF/1B9eWXr2Jjv7fSY/PK9Zf/vaMYv3aS+r3dtSk8tDZX79/WbH+mxecV6xPv/LBYj0bjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7D1g/Vc/Uqw/fvLXG7xCa2PpJeO+MqVY32fFT4r1izb+Wd3aD2+6vqme3nHEqWuK9df/af+6tcHNW1ra9q6IIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4ew+4/Yy/b/CM8v/JZz7zibq1p6/+peK6m04aKNZnrnikWG9k8s9eaGn9klsPva9Yn/vRs+vW9vw+4+wAdlOEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+wdsPXfPlSs/+rE8lj22oE3i/Vn/6b+6+91V/m312d+p1huq0bTQa966+1ifdakCcX6G2e9Ure25/eLq+6WGh7Zbd9ie4vtlSOWTbO9xPa62u3U9rYJoFVjeRv/DUkn7LDsIklLI2KmpKW1xwB6WMOwR8QDkrbusHiepMW1+4slnVJxXwAq1uwXdAdExCZJqt3W/bEv2wtt99vuH9D2JjcHoFVt/zY+IhZFRF9E9E3QpHZvDkAdzYZ9s+3pklS7zXcJEbCLaTbsd0taULu/QNJd1bQDoF0ajrPbvk3SsZL2tb1B0qWSLpd0h+0zJT0r6bR2Nrmrm39If7E+pCjWP/WvXy7WZzYYS++m2Fb/HIF7tv1icd2vPPiZYn3N79/YVE9ZNQx7RMyvU5pbcS8A2ojTZYEkCDuQBGEHkiDsQBKEHUiCS1wr8Pppc4r1M99X/qnon24vn1l4xD/Wv1RTkgaL1e4afPHFurUr/vKPi+uecOGKqttJjSM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOHsFBj6340/0vdseLu/mZds+WKwPrlqz0z3tCib/S/nS3Be+tF+HOsmBIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4+xiNmzKlbu24gx5v6bWv//fji/XD9ZOWXn9XteYHM8tPOOcHnWlkN8GRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9jMZNe1/d2qX7/UejtYvVia945xtKIBrsluteObxYn3bS2gq72fU1PLLbvsX2FtsrRyy7zPbztpfX/k5sb5sAWjWWt/HfkHTCKMuvjohZtb97q20LQNUahj0iHpBU/t0lAD2vlS/ozrX9aO1t/tR6T7K90Ha/7f4BbW9hcwBa0WzYb5B0uKRZkjZJurLeEyNiUUT0RUTfBJUnMATQPk2FPSI2R8RgRAxJulHS7GrbAlC1psJue/qIh6dKWlnvuQB6Q8Nxdtu3STpW0r62N0i6VNKxtmdJCknrJX2hjT32vCENtbS+o6JGdjNHHL+uWP/DKeVjzD2/88W6tXH/+d9N9bQraxj2iJg/yuKb29ALgDbidFkgCcIOJEHYgSQIO5AEYQeS4BJX9Kxjpj1ZrE8fv2exvn3qhLq18pq7J47sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+xjNPTiy3VrX97428V1rz7wR1W3s1sYf8D+xfqBE54p1jcNvlGsT3p5YKd72p1xZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnH6Ohbdvq1vq3zCyvfGDFzewm1n/+g8X6qXvdU6x/7JE/Ldb3/VG+n4su4cgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzl6BSTdNK9bHXVv+P/XCz95RrN9+/S8X64Ov/LxY76aNf/GxurUVZ/9Dg7XL++2N/9q3wfprG9RzaXhktz3D9v22V9teZfu82vJptpfYXle7ndr+dgE0ayxv49+WdEFEfFjSb0k6x/aRki6StDQiZkpaWnsMoEc1DHtEbIqIR2r3X5O0WtJBkuZJWlx72mJJp7SrSQCt26kv6GwfIukoScskHRARm6Th/xAkjfqDYrYX2u633T+g7a11C6BpYw677b0k3Snp/Ih4dazrRcSiiOiLiL4JmtRMjwAqMKaw256g4aDfGhHfrS3ebHt6rT5d0pb2tAigCg2H3mxb0s2SVkfEVSNKd0taIOny2u1dbelwFzDlwaeL9TtfLw8RzZ+yuVi/9Iry1yEfOqu/WG+nbafOKdbv/+Lf1a0NaWJx3U+vO7lYP+SOF4r1wWI1n7GMsx8t6QxJj9leXlt2sYZDfoftMyU9K+m09rQIoAoNwx4RP5bkOuW51bYDoF04XRZIgrADSRB2IAnCDiRB2IEkuMS1AoOby+cTXfLT8jj5p3/vpmJ92fHXFOsfveqCurUjrnm+uG4jm487uFhfcP69xfqUcfXH0m94pfwT3HHG+GJ98LmninW8G0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUjCEdGxje3taTHHXCi3o/X//GvF+qPH3NyhTt5rXIPjwZCGivX5T55Yt/bmWXsX1x1c+2SxjvdaFkv1amwd9SpVjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATXs/eAw/7kiWL943/0pWL9zr+q/9vs+40vz8Iz+6HPFuuv/XzPYn3S03sU6x/4Wv3ftI+BF4vroloc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiYbXs9ueIembkt4vaUjSooi4xvZlkj4v6Z3B0osjovgj4lzPDrRX6Xr2sZxU87akCyLiEdtTJD1se0mtdnVEXFFVowDaZyzzs2+StKl2/zXbqyUd1O7GAFRrpz6z2z5E0lGSltUWnWv7Udu32J5aZ52Ftvtt9w9oe0vNAmjemMNuey9Jd0o6PyJelXSDpMMlzdLwkf/K0daLiEUR0RcRfRNUPk8bQPuMKey2J2g46LdGxHclKSI2R8RgRAxJulHS7Pa1CaBVDcNu25JulrQ6Iq4asXz6iKedKmll9e0BqMpYvo0/WtIZkh6zvby27GJJ823PkhSS1kv6Qls6BFCJsXwb/2NJo43blSfmBtBTOIMOSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRMOfkq50Y/aLkp4ZsWhfSS91rIGd06u99WpfEr01q8rePhAR+41W6GjY37Nxuz8i+rrWQEGv9tarfUn01qxO9cbbeCAJwg4k0e2wL+ry9kt6tbde7Uuit2Z1pLeufmYH0DndPrID6BDCDiTRlbDbPsH2GttP2L6oGz3UY3u97cdsL7fd3+VebrG9xfbKEcum2V5ie13tdtQ59rrU22W2n6/tu+W2T+xSbzNs3297te1Vts+rLe/qviv01ZH91vHP7LbHS1or6ROSNkh6SNL8iPhZRxupw/Z6SX0R0fUTMGx/XNLrkr4ZEb9SW/a3krZGxOW1/yinRsSFPdLbZZJe7/Y03rXZiqaPnGZc0imSPqcu7rtCX59RB/ZbN47ssyU9ERFPRcRbkm6XNK8LffS8iHhA0tYdFs+TtLh2f7GG/7F0XJ3eekJEbIqIR2r3X5P0zjTjXd13hb46ohthP0jScyMeb1Bvzfceku6z/bDthd1uZhQHRMQmafgfj6T9u9zPjhpO491JO0wz3jP7rpnpz1vVjbCPNpVUL43/HR0RvyHpk5LOqb1dxdiMaRrvThllmvGe0Oz0563qRtg3SJox4vHBkjZ2oY9RRcTG2u0WSd9T701FvfmdGXRrt1u63M//6aVpvEebZlw9sO+6Of15N8L+kKSZtg+1PVHS6ZLu7kIf72F7cu2LE9meLOk49d5U1HdLWlC7v0DSXV3s5V16ZRrvetOMq8v7ruvTn0dEx/8knajhb+SflHRJN3qo09dhklbU/lZ1uzdJt2n4bd2Aht8RnSlpH0lLJa2r3U7rod6+JekxSY9qOFjTu9TbMRr+aPiopOW1vxO7ve8KfXVkv3G6LJAEZ9ABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBL/C9r5S+GZTvxaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[8].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8, grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[8].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
